par(opar)
demo(maath)
damo(plotmath)
help(plotmath)
with(mtcars, {
opar <- par(no.readonly = TRUE)
plot(wt, mpg, main = "Scatterplot of wt vs. mpg")
plot(wt, disp, main = "Scatterplot of wt vs. disp")
hist(wt, main = "Histogram of wt")
boxplot(wt, main = "Boxplot of wt")
par(opar)
})
with(mtcars, {
opar <- par(no.readonly = TRUE)
plot(wt, mpg, main = "Scatterplot of wt vs. mpg")
plot(wt, disp, main = "Scatterplot of wt vs. disp")
hist(wt, main = "Histogram of wt")
boxplot(wt, main = "Boxplot of wt")
par(opar)
})
with(mtcars, {
opar <- par(no.readonly = TRUE)
plot(wt, mpg, main = "Scatterplot of wt vs. mpg")
plot(wt, disp, main = "Scatterplot of wt vs. disp")
hist(wt, main = "Histogram of wt")
boxplot(wt, main = "Boxplot of wt")
par(opar)
})
with(mtcars, {
opar <- par(no.readonly = TRUE)
par(mfrow = c(2, 2))
plot(wt, mpg, main = "Scatterplot of wt vs. mpg")
plot(wt, disp, main = "Scatterplot of wt vs. disp")
hist(wt, main = "Histogram of wt")
boxplot(wt, main = "Boxplot of wt")
par(opar)
})
help(with)
x1 <- c(2, 2, 6, 4)
x2 <- c(3, 4, 2, 8)
mydata <- data.frame(x1, x2)
mydata$x1
with(mydata, {
mydata$SumX <- x1 + x2
mydata$MeanX <- (x1 + x2) / 2
})
mydata
with(mydata, {
mydata$SumX <- x1 + x2
mydata$MeanX <- (x1 + x2) / 2
})
with(mydata, {
mydata$SumX <<- x1 + x2
mydata$MeanX <<- (x1 + x2) / 2
})
mydata
with(mydata, {
plot(x1, x2)
})
?(cut)
help(cut)
mydates <- as.Date(c("2007-06-22", "2004-02-13"))
mydates
Sys.Date()
date()
help(as.date)
help(as.Date)
x <- c("1jan1960")
z <- as.Date(x, "%d%b%Y")
z
help
help(%in%)
%in%
help(subset)
1 : 4
1: 4
help(mean)
help(cat)
rep(1, 10)
Diabets <- c('Type1', 'Type2')
factor(Diabets)
install.packages(ISLR)
install.packages(MASS)
install.packages(ISLR)
install.packages(ISLR)
install.packages('ISLR')
par(mfrow = c(2, 2))
slices <- c(10, 12, 4, 16, 8)
lbls <- c('US', 'UK', 'Australia', 'Germany', 'France')
pie(slices, labels = lbls, main = "Simple Pie Chart")
pct <- round(slices / sum(slices) * 100)
lbls2 <- paste(lbls, ' ', pct, "%", sep = '')
lbls2
pie(slices, labels = lbls2, col = rainbow(length(lbls2)), main = "Pie Chart with Percentage")
library(plotrix)
pie3D(slices, labels = lbls, explode = 0.1, main = "3D Pie Chart")
mytable <- table(state.region)
mytable
lbls3 <- paste(names(mytable), '\n', mytable, sep = '')
pie(mytable, labels = lbls3, main = "Pie Chart from a Table\n (with sample sizes")
help(hist)
par(mfrow = c(2, 2))
hist{(mtcars$mpg)}
hist(mtcars$mpg)
hist(mtcars$mpg, breaks = 12, col = 'red')
hist(mtcars$mpg, breaks = 12, col = 'red')
rug(jitter(mtcars$mpg))
lines(density(mtcars$mpg), col = 'blue', lwd = 2)
x <- mtcars$mpg
h <- hist(x, breaks = 12, col = 'red')
xfit <- seq(min(x), max(x), length = 40)
yfit <- dnorm(xfit, mean = mean(x), sd = sd(x))
yfit <- yfit * diff(h$mids[1: 2]) * length(x)
lines(xfit, yfit, col = 'blue', lwd = 2)
box()
h&minds
h$mids
help(diff)
d <- density(mtcars$mpg)
polygon(d, col = 'red', border = 'blue')
plot(d)
polygon(d, col = 'red', border = 'blue')
help(polygon)
install('sm')
install.packages('sm')
par()
par(mfrow = c(1, 1))
library(sm)
attach(mtcars)
cyl.f <- factor(cyl, levels = c(4, 6, 8), labels = c('4 cylinder', '6 cylinder', '8 cylinder'))
sm.density.compare(mpg, cyl)
sm.density.compare(mpg, cyl.f)
cyl
sm.density.compare(mpg, cyl.f)
cyl
sm.density.compare(mpg, cyl)
par(mfrow = c(1, 1))
library(sm)
attach(mtcars)
cyl.f <- factor(cyl, levels = c(4, 6, 8), labels = c('4 cylinder', '6 cylinder', '8 cylinder'))
sm.density.compare(mpg, cyl)
colfill <- c(2: (1 + length(levels(cyl.f))))
legend(locator(1), levels(cyl.f), fill = colfill)
cyl.f
box(mpg ~ cyl, data = mtcars, notch = TRUE, varwidth = TRUE)
boxplot(mpg ~ cyl, data = mtcars, notch = TRUE, varwidth = TRUE)
install.packages('vioplot')
library(vioplot)
within(mtcars,{
x1 <- mpg[cyl == 4]
x2 <- mpg[cyl == 6]
x3 <- mpg[cyl == 8]
})
with(mtcars,{
x1 <<- mpg[cyl == 4]
x2 <<- mpg[cyl == 6]
x3 <<- mpg[cyl == 8]
})
within(mtcars,{
x1 <- mpg[cyl == 4]
x2 <- mpg[cyl == 6]
x3 <- mpg[cyl == 8]
})
vioplot(x1, x2, x3)
library(Hmisc)
myvars <- c('mpg', 'hp', 'wt')
describe(mtcars[myvars])
install.packages('pastecs')
library(pastecs)
myvar <- c('mpg', 'hp', 'wt')
stat.desc(mtcars[myvar])
install.packages('psych')
install.packages('doBy')
library(vcd)
install.packages('vcd')
install.packages("vcd")
library(vcd)
head(Arthritis)
install.packages('gmodels')
help(addmargins)
t_value <- 2.142884
2 * pt(t_value, 43)
2 * (1 - pt(t_value, 43))
qt(0.975, 43)
t_value <- 2.1429
pt(t_value, 43)
y <- c(40, 41, 43, 42, 44, 42, 43, 42)
x <- c(0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0)
m1 <- lm(y ~ x)
summary(m1)
anova(m1)
x_bar <- mean(x)
x_bar
ssx <- sum((x - x_bar) ** 2)
ssx
1.2877 / 10.5
sqrt(0.122638)
0.8842 * 0.8842
10.5 * 10.5
1.2877 * (1 / 8 + 110.25 / 10.5)
2.25 ** 2
1.2877 * (1 / 8 + 5.0625 / 10.5)
x <- c(0.3, 1.4, 1.0, -0.3, -0.2, 1.0, 2.0, -1.0, -0.7, 0.7)
y <- c(0.4, 0.9, 0.4, -0.3, 0.3, 0.8, 0.7, -0.4, -0.2, 0.7)
m1 <- lm(y ~ x)
summary(m1)
setwd("~/Coding/R")
census.data <- read.table("census.csv", header = T)
head(census.data)
census.data <- read.csv("census.csv", header = T)
head(census.data)
census.sample <- census.data[is.na(census.data) == FALSE]
head(census.sample)
census.sample <- census.data[is.na(census.data$outcome) == FALSE,]
head(census.sample)
census.prediction <- census.data[is.na(census.data$outcome) == TRUE,]
head(census.prediction)
census.data <- read.csv("census.csv", header = T)
with(census.data, {
educ <<- factor(educ)
status <<- factor(status)
race <<- factor(race)
gender <<- factor(gender)
})
census.sample <- census.data[is.na(census.data$outcome) == FALSE,]
census.prediction <- census.data[is.na(census.data$outcome) == TRUE,]
head(census.sample)
character(census.data$educ)
character(census.data$age)
help(character)
is.factor(census.data$educ)
logistic.fit = glm(outcome ~ age + educ + status + race + gender + hrs, data = census.sample, family = binomial)
summary(logistic.fit)
logistic.fit <- glm(outcome ~ age + educ + status + race + gender + hrs, data = census.sample, family = binomial)
summary(logistic.fit)
library(ISLR)
set.seed(1)
library(boot)
cv.err <- cv.glm(census.sample, logistic.fit)
dim(census.sample)
library(MASS)
help(qda)
qda.fit <- qda(outcome ~ age + educ + status + race + gender + hrs, data = census.sample)
summary(qda.fit)
predict(qda, newdata = data.frame(age = 10, educ = 'Bachelor', Status = 'Divorced', race = 'white', gender = 'Female'))
head(census.sample)
predict(qda, newdata = census.sample[1, 1 - 7], gender = 'Female'))
predict(qda, newdata = census.sample[1, c(1-7)], gender = 'Female'))
predict(qda, newdata = census.sample[1, c(1, 2, 3, 4, 5, 6)], gender = 'Female'))
predict(qda, newdata = census.sample[1, c(1, 2, 3, 4, 5, 6)])
predict(qda, newdata = census.sample[1, 1 : 6])
predict(qda, census.sample[1, 1 : 6])
predict(qda, census.sample[, 1:6])
predict(qda.fit, census.sample[, 1:6])
predict(qda.fit, census.sample[1, 1:6])
predict(qda.fit, census.sample[1:2, 1:6])
qda.class <- predict(qda.fit, census.sample[, 1:6])$class
table(qda.class, census.sample[, 7])
mean(qda.calss == census.sample[, 7])
mean(qda.class == census.sample[, 7])
logistic.fit <- predict(logistic.fit, census.sample[, 1:6])$class
predict(logistic.fit, census.sample[, 1:6])
predict(logistic.fit, census.sample[1, 1:6])
predict(logistic.fit, census.sample[1:2, 1:6])
logistic.fit <- glm(outcome ~ age + educ + status + race + gender + hrs, data = census.sample, family = binomial)
predict(logistic.fit, census.sample[1:2, 1:6])
logistic.fit <- glm(outcome ~ age + educ + status + race + gender + hrs, data = census.sample, family = "binomial")
predict(logistic.fit, census.sample[1:2, 1:6])
predict(logistic.fit, census.sample[1:10, 1:6])
predict(logistic.fit, census.sample[1:10, 1:6], type = 'response')
logistic.value <- predict(logistic.fit, census.sample[, 1:6], type = 'response')
mean(logistic.value)
logistic.value[logistic.value >= .5] = 1
logistic.value[logistic.value < .5] = 0
table(logistic.value, census.sample[, 7])
mean(logistic.value == census.sample[, 7])
step(logistic.fit, direction = "both")
step(logistic.fit, direction = "backward")
logistic.value <- predict(logistic.fit, census.prediction[, 1:6])
head(logistic.value)
logistic.value <- predict(logistic.fit, census.prediction[, 1:6], type = "response")
head(logistic.value)
logistic.value[logistic.value > .5] <- 1
logistic.value[logistic.value < .5] <- 0
head(logistic.value)
dim(logistic.value)
census.prediction$outcome <- logistic.value
output.data <- rbind(census.sample[, 7: 8], census.prediction[, 7: 8])
head(output.data)
output.data <- output.data[, c('id', 'outcome')]
head(output.data)
output.data <- output.data[order(output.data$id),]
head(output.data)
View(output.data)
predict(qda.fit, census.prediction[,1: 6])$class
predict(qda.fit, census.prediction[1,1: 6])$class
head(census.prediction)
predict(qda.fit, census.prediction[2,1: 6])$class
predict(qda.fit, census.prediction[3,1: 6])$class
predict(qda.fit, census.prediction[1:6,1: 6])$class
write.table(output.data, "hw5.txt")
write.table(output.data, "hw5.txt", sep = '\t')
View(output.data)
write.table(output.data, "hw5.txt", sep = '\t')
help(write.table)
dim(output.data)
write.table(output.data, "hw5.txt", sep = '\t')
write.table(output.data, "hw5.txt", col.names = c("id", "outcome"), row.names = output.data[,1] sep = '\t')
write.table(output.data, "hw5.txt", col.names = c("id", "outcome"), row.names = output.data[,1], sep = '\t')
write.table(output.data, "hw5.csv")
write.table(output.data, "hw5.csv", sep = '\t')
View(output.data)
write.csv(output.data, "hw5.csv")
input.data <- read.table("hw5.txt", header = T)
head(input.data)
View(census.data)
View(census.prediction)
census.data <- read.csv("census.csv", header = T)
with(census.data, {
educ <<- factor(educ)
status <<- factor(status)
race <<- factor(race)
gender <<- factor(gender)
})
census.sample <- census.data[is.na(census.data$outcome) == FALSE,]
census.prediction <- census.data[is.na(census.data$outcome) == TRUE,]
logistic.fit <- glm(outcome ~ age + educ + status + race + gender + hrs, data = census.sample, family = binomial)
logistic.value <- predict(logistic.fit, census.prediction[, 1:6], type = 'response')
logistic.value[logistic.value > .5] <- 1
logistic.value[logistic.value < .5] <- 0
output.data <- cbind(logistic.value, census.prediction[,8])
View(output.data)
write.table(logistc.value, 'output.txt', sep = '\t')
write.table(logistic.value, 'output.txt', sep = '\t')
write.table(logistic.value, 'output.txt', sep = '\t', colname = c('id', 'outcome'))
write.table(output.data, 'output.txt', sep = '\t')
output <- output[,c(2,1)]
output.data <- output.data[,c(2,1)]
write.table(output.data, 'output.txt', sep = '\t')
dim(output.data)
names(output.data) <- c('id', 'outcome')
write.table(output.data, 'output.txt', sep = '\t')
write.table(output.data, 'output.txt', sep = '\t')
library(FSelector)
if(!require(installr)) {
install.packages("installr"); require(installr)} #load / install+load installr
# using the package:
updateR()
updateR()
??updateR
library(installr)
install.packages('dplyr')
help(dplyr)
install.packages('knitr')
vignette('datatable-intro')
install.packages(plotly)
install.packages('plotly')
??boxcox
??dpyr
help(pt)
setwd("~/Courses/Statistical-Machine-Learning/HW3")
train_x <- read.table('uspsdata.txt', header = FALSE)
train_y <- read.table('uspscl.txt', header = FALSE)
# The train function
train <- function(X, w, y){
if (!is.vector(y)){
y <- as.vector(y$V1)
}
n <- dim(X)[1]   # the number of observations
d <- dim(X)[2]   # the number of features
theta <- c()
error_rate <- c()
m_list <- c()
# compute the error rate for a weak learner
learning_rule <- function(theta_j, x_j, weight, m_stump){
predict_y <- c()
# predict the classification based on the split point
for (i in c(1: n)){
if (x_j[i] > theta_j){
predict_y[i] <- m_stump
} else {
predict_y[i] <- -m_stump
}
}
error_rate <- 0
for (i in c(1: n)){
if (y[i] != predict_y[i]){
error_rate <- error_rate + weight[i]
}
}
error_rate <- error_rate / (sum(weight))
return(error_rate)
}
# compute the optimal parameter theta_j and m for each x
for (j in c(1: d)){
test_parameter <- c()
test_error_rate <- c()
k <- 1
# compute the arg min of cost function
for (m in c(-1, 1)){
optimization <- optimize(learning_rule, interval = c(min(X[,j]), max(X[,j])), x_j = X[, j], weight = w, m_stump = m)
test_parameter[k] <- optimization$minimum
test_error_rate[k] <- optimization$objective
k <- k + 1
}
if(test_error_rate[1] < test_error_rate[2]){
theta[j] <- test_parameter[1]
error_rate[j] <- test_error_rate[1]
m_list[j] <- c(-1)
} else {
theta[j] <- test_parameter[2]
error_rate[j] <- test_error_rate[2]
m_list[j] <- c(1)
}
}
location_j <- which.min(error_rate)
result_list = list(j = location_j, theta = theta[location_j], mode = m_list[location_j])
return(result_list)
}
# The classify function
classify <- function(X, pars){
label <- (2*(X[, pars$j] > pars$theta) - 1) * pars$mode
return(label)
}
# The adaBoost function
adaBoost <- function(X, y, B){
if (!is.vector(y)){
y <- as.vector(y$V1)
}
n <- dim(X)[1]
d <- dim(X)[2]
weight_list <- rep(1/n, d)
alpha <- c()
allPars <- list()
#train routin
for (i in c(1: B)){
allPars[[i]] <- train(X, weight_list, y)
predict_y <- classify(X, allPars[[i]])
error_rate <- sum((predict_y != y) * weight_list) / (sum(weight_list))
alpha[i] <- log((1 - error_rate) / error_rate)
weight_list <- weight_list * exp(alpha[i] * (predict_y != y))
}
return(list(alpha = alpha, allPars = allPars))
}
# agg_class function
agg_class <- function(X, alpha, allPars) {
n <- dim(X)[1]
B <- length(alpha)
labels <- matrix(0, nrow = n, ncol = B)
for (i in c(1: B)){
labels[,i] <- classify(X, allPars[[i]])
}
sum_label <- labels %*% alpha
classifier <- as.vector(sign(sum_label))
return(classifier)
}
# cross validation function
cross_validation <- function(X, y, B_max, k_fold){
n <- dim(X)[1]
train_error_rate <- matrix(0, nrow = B_max, ncol = k_fold)
test_error_rate <- matrix(0, nrow = B_max, ncol = k_fold)
# split data into k group
num_of_group <- round(n/k_fold)
k_fold_groups <- list()
for(k in 1:k_fold){
ini_point <- (k - 1) * num_of_group
stop_point <- k * num_of_group
if(stop_point < n){
k_fold_groups[[k]] <- c(ini_point: stop_point)
} else {
k_fold_groups[[k]] <- c(ini_point : n)
}
}
# for each iteration in cross validation, choose train data and test data
for(k in 1:k_fold){
train.x <- X[-k_fold_groups[[k]],]
train.y <- y[-k_fold_groups[[k]],]
test.x <- X[k_fold_groups[[k]],]
test.y <- y[k_fold_groups[[k]],]
ada <- adaBoost(train.x, train.y, B_max)
allPars <- ada$allPars
alpha <- ada$alpha
# compute test error rate and train error rate for each b
for (b in 1: B_max){
test_predict_y <- agg_class(test.x, alpha[1:b], allPars = allPars[1:b])
test_error_rate[b, k] <- mean(test.y != test_predict_y)
train_predict_y <- agg_class(train.x, alpha[1:b], allPars = allPars[1:b])
train_error_rate[b, k] <- mean(train.y != train_predict_y)
}
}
return(list(train_error_rate = train_error_rate, test_error_rate = test_error_rate))
}
result <- cross_validation(train_x, train_y, 60, 5)
result <- cross_validation(train_x, train_y, 60, 5)
matplot(result$train_error_rate, type = 'l', lty = 1: 5, main = 'Training Error', xlab = 'number of base classifier',
ylab = 'error rate')
matplot(result$test_error_rate, type = 'l', lty = 1: 5, main = 'Test Error', xlab = 'number of base classifier',
ylab = 'error rate')
matplot(result$train_error_rate, type = 'l', lty = 1: 5, main = 'Training Error', xlab = 'number of base classifier',
ylab = 'error rate', ylim = c(0, 0.5))
matplot(result$test_error_rate, type = 'l', lty = 1: 5, main = 'Test Error', xlab = 'number of base classifier',
ylab = 'error rate', , ylim = c(0, 0.5))
matplot(result$train_error_rate, type = 'l', lty = 1: 5, main = 'Training Error', xlab = 'number of base classifier',
ylab = 'error rate', ylim = c(0, 0.3))
matplot(result$test_error_rate, type = 'l', lty = 1: 5, main = 'Test Error', xlab = 'number of base classifier',
ylab = 'error rate', ylim = c(0, 0.3))
matplot(result$train_error_rate, type = 'l', lty = 1: 5, main = 'Training Error', xlab = 'number of base classifier',
ylab = 'error rate', ylim = c(0, 0.2))
matplot(result$test_error_rate, type = 'l', lty = 1: 5, main = 'Test Error', xlab = 'number of base classifier',
ylab = 'error rate', ylim = c(0, 0.2))
save(result, 'Result.Rdata')
save(result, file = 'Result.Rdata')
