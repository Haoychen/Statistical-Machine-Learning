vignette('datatable-intro')
install.packages(plotly)
install.packages('plotly')
??boxcox
??dpyr
help(pt)
setwd("~/Courses/Statistical-Machine-Learning/HW3")
train.x <- read.table('uspsdata.txt', header = FALSE)
train.y <- read.table('uspscl.txt', header = FALSE)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
w <- rep(1/200, 256)
train(train.x, w, train.y)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(train.x, )
train(train.x, w, train.y)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(train.x, w, train.y)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(train.x, w, train.y)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(train.x, w, train.y)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(train.x, w, train.y)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(train.x, w, train.y)
learning_rule <- function(theta_j, x_j, weight, m_stump){
predict_y <- c()
for (i in c(1: n)){
if (x_j > theta_j){
predict_y[i] <- m_stump
} else {
predict_y[i] <- -m_stump
}
}
error_rate <- 0
for (i in c(1: n)){
if (y[i] != predict_y[i]){
error_rate <- error_rate + weight[i]
}
}
error_rate <- error_rate / (sum(weight))
return(error_rate)
}
learning_rule(15, X[, 25], w, 1)
learning_rule(15, train.x[, 25], w, 1)
learning_rule(15, train.x, w, 1)
learning_rule(15, train.x[, 15], w, 1)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
learning_rule(15, train.x[, 15], w, 1)
n <- dim(X)[1]   # the number of observations
d <- dim(X)[2]   # the number of features
n <- dim(train.x)[1]   # the number of observations
d <- dim(X)[2]   # the number of features
n <- dim(train.x)[1]   # the number of observations
d <- dim(train.x)[2]   # the number of features
y <- as.vector(train.y$V1)
learning_rule(15, train.x[, 15], w, 1)
learning_rule(15, train.x[, 15], w, -1)
learning_rule <- function(theta_j, x_j, weight, m_stump){
predict_y <- c()
for (i in c(1: n)){
if (x_j > theta_j){
predict_y[i] <- m_stump
} else {
predict_y[i] <- -m_stump
}
}
print(predict_y)
error_rate <- 0
for (i in c(1: n)){
if (y[i] != predict_y[i]){
error_rate <- error_rate + weight[i]
}
}
error_rate <- error_rate / (sum(weight))
return(error_rate)
}
learning_rule(15, train.x[, 15], w, -1)
learning_rule(15, train.x[, 15], w, 1)
learning_rule <- function(theta_j, x_j, weight, m_stump){
predict_y <- c()
for (i in c(1: n)){
if (x_j > theta_j){
predict_y[i] <- m_stump
} else {
predict_y[i] <- -m_stump
}
}
error_rate <- 0
print(y == predict_y)
for (i in c(1: n)){
if (y[i] != predict_y[i]){
error_rate <- error_rate + weight[i]
}
}
error_rate <- error_rate / (sum(weight))
return(error_rate)
}
learning_rule(15, train.x[, 15], w, -1)
learning_rule(15, train.x[, 15], w, 1)
# The train function
train <- function(X, w, y){
y <- as.vector(y$V1)
n <- dim(X)[1]   # the number of observations
d <- dim(X)[2]   # the number of features
theta <- c()
error_rate <- c()
m <- c()
# compute the error rate for a weak learner
learning_rule <- function(theta_j, x_j, weight, m_stump){
predict_y <- c()
for (i in c(1: n)){
if (x_j > theta_j){
predict_y[i] <- m_stump
} else {
predict_y[i] <- -m_stump
}
}
error_rate <- 0
print(y == predict_y)
print(sum(y == predict_y))
for (i in c(1: n)){
if (y[i] != predict_y[i]){
error_rate <- error_rate + weight[i]
}
}
error_rate <- error_rate / (sum(weight))
return(error_rate)
}
learning_rule(15, train.x[, 15], w, -1)
learning_rule <- function(theta_j, x_j, weight, m_stump){
predict_y <- c()
for (i in c(1: n)){
if (x_j > theta_j){
predict_y[i] <- m_stump
} else {
predict_y[i] <- -m_stump
}
}
error_rate <- 0
print(y == predict_y)
print(sum(y == predict_y))
for (i in c(1: n)){
if (y[i] != predict_y[i]){
error_rate <- error_rate + weight[i]
}
}
error_rate <- error_rate / (sum(weight))
return(error_rate)
}
learning_rule(15, train.x[, 15], w, -1)
learning_rule(15, train.x[, 15], w, 1)
learning_rule(0.2, train.x[, 15], w, 1)
learning_rule(0.1, train.x[, 15], w, 1)
learning_rule(0.01, train.x[, 15], w, 1)
View(train.x)
learning_rule <- function(theta_j, x_j, weight, m_stump){
predict_y <- c()
for (i in c(1: n)){
if (x_j[i] > theta_j){
predict_y[i] <- m_stump
} else {
predict_y[i] <- -m_stump
}
}
error_rate <- 0
print(y == predict_y)
print(sum(y == predict_y))
for (i in c(1: n)){
if (y[i] != predict_y[i]){
error_rate <- error_rate + weight[i]
}
}
error_rate <- error_rate / (sum(weight))
return(error_rate)
}
learning_rule(15, train.x[, 15], w, -1)
learning_rule(15, train.x[, 15], w, 1)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train.x <- read.table('uspsdata.txt', header = FALSE)
train.y <- read.table('uspscl.txt', header = FALSE)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
w <- rep(1/200, 256)
train(train.x, w, train.y)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
learning_rule(15, train.x[, 15], w, -1)
train(train.x, w, train.y)
train <- function(X, w, y){
y <- as.vector(y$V1)
n <- dim(X)[1]   # the number of observations
d <- dim(X)[2]   # the number of features
theta <- c()
error_rate <- c()
m <- c()
# compute the error rate for a weak learner
learning_rule <- function(theta_j, x_j, weight, m_stump){
predict_y <- c()
for (i in c(1: n)){
if (x_j[i] > theta_j){
predict_y[i] <- m_stump
} else {
predict_y[i] <- -m_stump
}
}
error_rate <- 0
for (i in c(1: n)){
if (y[i] != predict_y[i]){
error_rate <- error_rate + weight[i]
}
}
error_rate <- error_rate / (sum(weight))
return(error_rate)
}
# compute the optimal parameter theta_j and m for each x
for (j in c(1: d)){
test_parameter <- c()
test_error_rate <- c()
for (m in c(-1, 1)){
optimization <- optimize(learning_rule, interval = c(min(X[,j]), max(X[,j])), x_j = X[, j], weight = w, m_stump = m)
test_parameter <- union(test_parameter, optimization$minimum)
test_error_rate <- union(test_error_rate, optimization$objective)
}
if(test_error_rate[1] < test_error_rate[2]){
theta[j] <- test_parameter[1]
error_rate[j] <- test_error_rate[1]
m[j] <- -1
} else {
theta[j] <- test_parameter[2]
error_rate[j] <- test_error_rate[2]
m[j] <- 1
}
}
location_j <- which.min(error_rate)
return(c(location_j, theta[location_j], error_rate[location_j]))
}
train(train.x, w, train.y)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(train.x, w, train.y)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(train.x, w, train.y)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(train.x, w, train.y)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(train.x, w, train.y)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(train.x, w, train.y)
print(error_rate)
View(train.x)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(train.x, w, train.y)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(train.x, w, train.y)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(train.x, w, train.y)
View(train.x)
train.x$V141
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(train.x, w, train.y)
learning_rule <- function(theta_j, x_j, weight, m_stump){
predict_y <- c()
for (i in c(1: n)){
if (x_j[i] > theta_j){
predict_y[i] <- m_stump
} else {
predict_y[i] <- -m_stump
}
}
error_rate <- 0
for (i in c(1: n)){
if (y[i] != predict_y[i]){
error_rate <- error_rate + weight[i]
}
}
error_rate <- error_rate / (sum(weight))
return(error_rate)
}
y <- as.vector(train.y$V1)
n <- dim(train.x)[1]   # the number of observations
d <- dim(train.x)[2]
w <- rep(1/200, 256)
optimize(learning_rule, interval = c(min(X[,j]), max(train.x[,141])), x_j = train.x[, 141], weight = w, m_stump = 1)
optimize(learning_rule, interval = c(min(train.x[,j]), max(train.x[,141])), x_j = train.x[, 141], weight = w, m_stump = 1)
optimize(learning_rule, interval = c(min(train.x[,141]), max(train.x[,141])), x_j = train.x[, 141], weight = w, m_stump = 1)
optimize(learning_rule, interval = c(min(train.x[,141]), max(train.x[,141])), x_j = train.x[, 141], weight = w, m_stump = -1)
train.x <- read.table('uspsdata.txt', header = FALSE)
train.y <- read.table('uspscl.txt', header = FALSE)
# The train function
train <- function(X, w, y){
y <- as.vector(y$V1)
n <- dim(X)[1]   # the number of observations
d <- dim(X)[2]   # the number of features
theta <- c()
error_rate <- c()
m <- c()
# compute the error rate for a weak learner
learning_rule <- function(theta_j, x_j, weight, m_stump){
predict_y <- c()
for (i in c(1: n)){
if (x_j[i] > theta_j){
predict_y[i] <- m_stump
} else {
predict_y[i] <- -m_stump
}
}
error_rate <- 0
for (i in c(1: n)){
if (y[i] != predict_y[i]){
error_rate <- error_rate + weight[i]
}
}
error_rate <- error_rate / (sum(weight))
return(error_rate)
}
# compute the optimal parameter theta_j and m for each x
for (j in c(1: d)){
print(j)
test_parameter <- c()
test_error_rate <- c()
for (m in c(-1, 1)){
optimization <- optimize(learning_rule, interval = c(min(X[,j]), max(X[,j])), x_j = X[, j], weight = w, m_stump = m)
test_parameter <- union(test_parameter, optimization$minimum)
test_error_rate <- union(test_error_rate, optimization$objective)
}
if(test_error_rate[1] < test_error_rate[2]){
theta[j] <- test_parameter[1]
error_rate[j] <- test_error_rate[1]
m[j] <- -1
} else {
theta[j] <- test_parameter[2]
error_rate[j] <- test_error_rate[2]
m[j] <- 1
}
}
location_j <- which.min(error_rate)
return(c(location_j, theta[location_j], error_rate[location_j]))
}
w <- rep(1/200, 256)
train(tran.x, w, train.y)
train(train.x, w, train.y)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(tran.x, w, train.y)
train(train.x, w, train.y)
# The train function
train <- function(X, w, y){
y <- as.vector(y$V1)
n <- dim(X)[1]   # the number of observations
d <- dim(X)[2]   # the number of features
theta <- c()
error_rate <- c()
m <- c()
# compute the error rate for a weak learner
learning_rule <- function(theta_j, x_j, weight, m_stump){
predict_y <- c()
for (i in c(1: n)){
if (x_j[i] > theta_j){
predict_y[i] <- m_stump
} else {
predict_y[i] <- -m_stump
}
}
error_rate <- 0
for (i in c(1: n)){
if (y[i] != predict_y[i]){
error_rate <- error_rate + weight[i]
}
}
error_rate <- error_rate / (sum(weight))
return(error_rate)
}
# compute the optimal parameter theta_j and m for each x
for (j in c(1: d)){
print(j)
test_parameter <- c()
test_error_rate <- c()
k <- 1
for (m in c(-1, 1)){
optimization <- optimize(learning_rule, interval = c(min(X[,j]), max(X[,j])), x_j = X[, j], weight = w, m_stump = m)
test_parameter[k] <- optimization$minimum
test_error_rate[k] <- optimization$objective
k <- k + 1
}
if(test_error_rate[1] < test_error_rate[2]){
theta[j] <- test_parameter[1]
error_rate[j] <- test_error_rate[1]
m[j] <- -1
} else {
theta[j] <- test_parameter[2]
error_rate[j] <- test_error_rate[2]
m[j] <- 1
}
}
location_j <- which.min(error_rate)
return(c(location_j, theta[location_j], error_rate[location_j]))
}
train(train.x, w, train.y)
# The train function
train <- function(X, w, y){
y <- as.vector(y$V1)
n <- dim(X)[1]   # the number of observations
d <- dim(X)[2]   # the number of features
theta <- c()
error_rate <- c()
m <- c()
# compute the error rate for a weak learner
learning_rule <- function(theta_j, x_j, weight, m_stump){
predict_y <- c()
for (i in c(1: n)){
if (x_j[i] > theta_j){
predict_y[i] <- m_stump
} else {
predict_y[i] <- -m_stump
}
}
error_rate <- 0
for (i in c(1: n)){
if (y[i] != predict_y[i]){
error_rate <- error_rate + weight[i]
}
}
error_rate <- error_rate / (sum(weight))
return(error_rate)
}
# compute the optimal parameter theta_j and m for each x
for (j in c(1: d)){
test_parameter <- c()
test_error_rate <- c()
k <- 1
for (m in c(-1, 1)){
optimization <- optimize(learning_rule, interval = c(min(X[,j]), max(X[,j])), x_j = X[, j], weight = w, m_stump = m)
test_parameter[k] <- optimization$minimum
test_error_rate[k] <- optimization$objective
k <- k + 1
}
if(test_error_rate[1] < test_error_rate[2]){
theta[j] <- test_parameter[1]
error_rate[j] <- test_error_rate[1]
m[j] <- -1
} else {
theta[j] <- test_parameter[2]
error_rate[j] <- test_error_rate[2]
m[j] <- 1
}
}
location_j <- which.min(error_rate)
return(c(location_j, theta[location_j], error_rate[location_j]))
}
train(train.x, w, train.y)
# The train function
train <- function(X, w, y){
y <- as.vector(y$V1)
n <- dim(X)[1]   # the number of observations
d <- dim(X)[2]   # the number of features
theta <- c()
error_rate <- c()
m <- c()
# compute the error rate for a weak learner
learning_rule <- function(theta_j, x_j, weight, m_stump){
predict_y <- c()
for (i in c(1: n)){
if (x_j[i] > theta_j){
predict_y[i] <- m_stump
} else {
predict_y[i] <- -m_stump
}
}
error_rate <- 0
for (i in c(1: n)){
if (y[i] != predict_y[i]){
error_rate <- error_rate + weight[i]
}
}
error_rate <- error_rate / (sum(weight))
return(error_rate)
}
# compute the optimal parameter theta_j and m for each x
for (j in c(1: d)){
test_parameter <- c()
test_error_rate <- c()
k <- 1
for (m in c(-1, 1)){
optimization <- optimize(learning_rule, interval = c(min(X[,j]), max(X[,j])), x_j = X[, j], weight = w, m_stump = m)
test_parameter[k] <- optimization$minimum
test_error_rate[k] <- optimization$objective
k <- k + 1
}
if(test_error_rate[1] < test_error_rate[2]){
theta[j] <- test_parameter[1]
error_rate[j] <- test_error_rate[1]
m[j] <- -1
} else {
theta[j] <- test_parameter[2]
error_rate[j] <- test_error_rate[2]
m[j] <- 1
}
}
location_j <- as.integer(which.min(error_rate))
return(c(location_j, theta[location_j], m[location_j]))
}
train(train.x, w, train.y)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(train.x, w, train.y)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(train.x, w, train.y)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(train.x, w, train.y)
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
debugSource('~/Courses/Statistical-Machine-Learning/HW3/HW3-1.R')
train(train.x, w, train.y)
train(train.x, w, train.y)
